"""
å†…å®¹åˆ†æå™¨
åˆ†æå¸–å­å†…å®¹çš„åˆ†ç±»ã€æƒ…æ„Ÿã€å…³é”®è¯ç­‰
"""

import jieba
import jieba.analyse
import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class ContentAnalysis:
    """å†…å®¹åˆ†æç»“æœ"""
    category: str
    sentiment: str  # positive, negative, neutral
    keywords: List[str]
    topics: List[str]
    complexity: str  # simple, medium, complex
    intent: str  # question, share, discussion, help
    language_style: str  # formal, casual, technical
    confidence: float


class ContentAnalyzer:
    """å†…å®¹åˆ†æå™¨"""
    
    def __init__(self):
        # åˆå§‹åŒ–jieba
        jieba.initialize()
        
        # é¢„å®šä¹‰åˆ†ç±»å…³é”®è¯ï¼ˆä¼˜åŒ–æƒé‡ï¼‰
        self.category_keywords = {
            "æ±‚åŠ©é—®ç­”": {
                "high": ["æ±‚åŠ©", "å¸®å¿™", "è¯·æ•™", "æ•‘å‘½", "æ€¥", "ä¸ä¼š", "åäº†", "å‡ºé—®é¢˜", "æ•…éšœ"],
                "medium": ["é—®é¢˜", "æ€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "é”™è¯¯", "bug", "è§£å†³", "ä¿®å¤"],
                "low": ["å¸®åŠ©", "æŒ‡å¯¼", "å»ºè®®"]
            },
            "æŠ€æœ¯è®¨è®º": {
                "high": ["æŠ€æœ¯", "ä»£ç ", "ç¼–ç¨‹", "å¼€å‘", "ç®—æ³•", "æ¡†æ¶"],
                "medium": ["API", "æ•°æ®åº“", "æœåŠ¡å™¨", "å‰ç«¯", "åç«¯", "æ¶æ„"],
                "low": ["å®ç°", "é…ç½®", "éƒ¨ç½²"]
            },
            "ç”Ÿæ´»åˆ†äº«": {
                "high": ["åˆ†äº«", "æ¨è", "ä½“éªŒ", "æ„Ÿå—"],
                "medium": ["ç”Ÿæ´»", "æ—¥å¸¸", "å¿ƒæƒ…"],
                "low": ["ä»Šå¤©", "æ˜¨å¤©", "æœ€è¿‘"]
            },
            "æ–°é—»èµ„è®¯": {
                "high": ["æ–°é—»", "èµ„è®¯", "å‘å¸ƒ", "æ›´æ–°", "å…¬å‘Š"],
                "medium": ["é€šçŸ¥", "æ¶ˆæ¯", "æŠ¥é“"],
                "low": ["æœ€æ–°", "å®˜æ–¹"]
            },
            "è®¨è®ºäº¤æµ": {
                "high": ["è®¨è®º", "äº¤æµ", "è§‚ç‚¹", "çœ‹æ³•"],
                "medium": ["æ„è§", "æƒ³æ³•", "è®¤ä¸º", "è§‰å¾—"],
                "low": ["æ€è€ƒ", "è€ƒè™‘"]
            },
            "èµ„æºåˆ†äº«": {
                "high": ["èµ„æº", "ä¸‹è½½", "é“¾æ¥", "å·¥å…·"],
                "medium": ["è½¯ä»¶", "æ•™ç¨‹", "æ–‡æ¡£", "èµ„æ–™"],
                "low": ["æ”¶é›†", "æ•´ç†"]
            }
        }
        
        # æƒ…æ„Ÿè¯å…¸ï¼ˆæ‰©å±•ï¼‰
        self.positive_words = ["å¥½", "æ£’", "èµ", "ä¼˜ç§€", "å®Œç¾", "å–œæ¬¢", "æ»¡æ„", "æ¨è", "å‰å®³", "ä¸é”™", "æˆåŠŸ", "è§£å†³äº†", "æœ‰ç”¨", "æ„Ÿè°¢"]
        self.negative_words = ["å·®", "çƒ‚", "ç³Ÿç³•", "å¤±æœ›", "è®¨åŒ", "é—®é¢˜", "é”™è¯¯", "bug", "éš¾ç”¨", "åƒåœ¾", "åäº†", "æ•…éšœ", "ä¸è¡Œ", "æ²¡ååº”", "å‡ºé—®é¢˜", "æ±‚åŠ©", "æ•‘å‘½", "æ€¥"]
        
        # æ„å›¾è¯†åˆ«æ¨¡å¼
        self.intent_patterns = {
            "question": [r"[ï¼Ÿ?]", r"æ€ä¹ˆ", r"å¦‚ä½•", r"ä¸ºä»€ä¹ˆ", r"ä»€ä¹ˆ", r"å“ªé‡Œ", r"è°"],
            "help": [r"æ±‚åŠ©", r"å¸®å¿™", r"è¯·æ•™", r"ä¸ä¼š", r"æ•‘å‘½"],
            "share": [r"åˆ†äº«", r"æ¨è", r"ä»‹ç»", r"ç»™å¤§å®¶"],
            "discussion": [r"è®¨è®º", r"çœ‹æ³•", r"è§‚ç‚¹", r"æ„è§", r"è®¤ä¸º"]
        }
        
        # æŠ€æœ¯è¯æ±‡
        self.tech_words = [
            "Python", "JavaScript", "Java", "C++", "React", "Vue", "Node.js", 
            "Docker", "Kubernetes", "MySQL", "Redis", "MongoDB", "Git", "Linux"
        ]
    
    def analyze(self, title: str, content: str) -> ContentAnalysis:
        """åˆ†æå†…å®¹"""
        full_text = f"{title} {content}"
        
        # åˆ†ç±»è¯†åˆ«
        category = self._classify_content(full_text)
        
        # æƒ…æ„Ÿåˆ†æ
        sentiment = self._analyze_sentiment(full_text)
        
        # å…³é”®è¯æå–
        keywords = self._extract_keywords(full_text)
        
        # ä¸»é¢˜æå–
        topics = self._extract_topics(full_text)
        
        # å¤æ‚åº¦è¯„ä¼°
        complexity = self._assess_complexity(full_text)
        
        # æ„å›¾è¯†åˆ«
        intent = self._identify_intent(full_text)
        
        # è¯­è¨€é£æ ¼
        language_style = self._analyze_language_style(full_text)
        
        # ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(full_text, category, sentiment)
        
        return ContentAnalysis(
            category=category,
            sentiment=sentiment,
            keywords=keywords,
            topics=topics,
            complexity=complexity,
            intent=intent,
            language_style=language_style,
            confidence=confidence
        )
    
    def _classify_content(self, text: str) -> str:
        """å†…å®¹åˆ†ç±»ï¼ˆä½¿ç”¨æƒé‡ç³»ç»Ÿï¼‰"""
        scores = {}
        text_lower = text.lower()
        
        # æƒé‡è®¾ç½®
        weights = {"high": 3, "medium": 2, "low": 1}
        
        for category, keyword_groups in self.category_keywords.items():
            score = 0
            for weight_level, keywords in keyword_groups.items():
                weight = weights[weight_level]
                for keyword in keywords:
                    count = text_lower.count(keyword.lower())
                    score += count * weight
            scores[category] = score
        
        # ç‰¹æ®Šè§„åˆ™ï¼šæ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡åŠ å€
        title_end = text.find(' ')
        if title_end > 0:
            title = text[:title_end].lower()
            for category, keyword_groups in self.category_keywords.items():
                for weight_level, keywords in keyword_groups.items():
                    weight = weights[weight_level]
                    for keyword in keywords:
                        if keyword.lower() in title:
                            scores[category] += weight * 2
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
        if scores and max(scores.values()) > 0:
            return max(scores, key=scores.get)
        return "è®¨è®ºäº¤æµ"
    
    def _analyze_sentiment(self, text: str) -> str:
        """æƒ…æ„Ÿåˆ†æ"""
        positive_count = sum(text.count(word) for word in self.positive_words)
        negative_count = sum(text.count(word) for word in self.negative_words)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def _extract_keywords(self, text: str, top_k: int = 5) -> List[str]:
        """æå–å…³é”®è¯"""
        try:
            # ä½¿ç”¨jiebaæå–å…³é”®è¯
            keywords = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)
            return keywords
        except Exception:
            # ç®€å•çš„å…³é”®è¯æå–é™çº§æ–¹æ¡ˆ
            words = jieba.cut(text)
            word_freq = {}
            for word in words:
                if len(word) > 1 and word not in ['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'éƒ½', 'è€Œ', 'åŠ']:
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            return [word for word, freq in sorted_words[:top_k]]
    
    def _extract_topics(self, text: str) -> List[str]:
        """æå–ä¸»é¢˜"""
        topics = []
        text_lower = text.lower()
        
        # æŠ€æœ¯ç›¸å…³ä¸»é¢˜
        for topic in self.tech_words:
            if topic.lower() in text_lower:
                topics.append(topic)
        
        # å…¶ä»–ä¸»é¢˜è¯†åˆ«
        topic_patterns = {
            "å‰ç«¯å¼€å‘": ["å‰ç«¯", "html", "css", "javascript", "react", "vue"],
            "åç«¯å¼€å‘": ["åç«¯", "æœåŠ¡å™¨", "æ•°æ®åº“", "api", "æ¥å£"],
            "ç§»åŠ¨å¼€å‘": ["ç§»åŠ¨", "app", "android", "ios", "flutter"],
            "äººå·¥æ™ºèƒ½": ["ai", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ"],
            "åŒºå—é“¾": ["åŒºå—é“¾", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ", "æ™ºèƒ½åˆçº¦"]
        }
        
        for topic, keywords in topic_patterns.items():
            if any(keyword in text_lower for keyword in keywords):
                topics.append(topic)
        
        return topics[:3]  # æœ€å¤šè¿”å›3ä¸ªä¸»é¢˜
    
    def _assess_complexity(self, text: str) -> str:
        """è¯„ä¼°å†…å®¹å¤æ‚åº¦"""
        # åŸºäºæ–‡æœ¬é•¿åº¦å’ŒæŠ€æœ¯è¯æ±‡å¯†åº¦
        length = len(text)
        tech_count = sum(1 for word in self.tech_words if word.lower() in text.lower())
        
        # è®¡ç®—æŠ€æœ¯è¯æ±‡å¯†åº¦
        tech_density = tech_count / max(length / 100, 1)  # æ¯100å­—çš„æŠ€æœ¯è¯æ±‡æ•°
        
        if length > 500 or tech_density > 3:
            return "complex"
        elif length > 200 or tech_density > 1:
            return "medium"
        else:
            return "simple"
    
    def _identify_intent(self, text: str) -> str:
        """è¯†åˆ«ç”¨æˆ·æ„å›¾"""
        for intent, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text):
                    return intent
        return "discussion"
    
    def _analyze_language_style(self, text: str) -> str:
        """åˆ†æè¯­è¨€é£æ ¼"""
        formal_indicators = ["æ‚¨", "è¯·", "è°¢è°¢", "ä¸å¥½æ„æ€", "éº»çƒ¦", "æ‰“æ‰°"]
        casual_indicators = ["å“ˆå“ˆ", "å—¯", "å‘€", "å•Š", "å“¦", "é¢"]
        technical_indicators = ["å®ç°", "é…ç½®", "éƒ¨ç½²", "ä¼˜åŒ–", "æ¶æ„", "ç®—æ³•"]
        
        formal_count = sum(text.count(word) for word in formal_indicators)
        casual_count = sum(text.count(word) for word in casual_indicators)
        technical_count = sum(text.count(word) for word in technical_indicators)
        
        if technical_count > max(formal_count, casual_count):
            return "technical"
        elif formal_count > casual_count:
            return "formal"
        else:
            return "casual"
    
    def _calculate_confidence(self, text: str, category: str, sentiment: str) -> float:
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        base_confidence = 0.5
        
        # æ–‡æœ¬é•¿åº¦åŠ åˆ†
        if len(text) > 50:
            base_confidence += 0.2
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        if category in self.category_keywords:
            keywords = self.category_keywords[category]
            matches = sum(1 for keyword in keywords if keyword in text.lower())
            base_confidence += min(matches * 0.05, 0.3)
        
        # æƒ…æ„Ÿè¯åŒ¹é…åŠ åˆ†
        emotion_words = self.positive_words + self.negative_words
        emotion_matches = sum(1 for word in emotion_words if word in text)
        if emotion_matches > 0:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)
    
    def get_reply_suggestions(self, analysis: ContentAnalysis) -> Dict[str, List[str]]:
        """æ ¹æ®åˆ†æç»“æœè·å–å›å¤å»ºè®®"""
        suggestions = {
            "short_replies": [],
            "emoji_suggestions": [],
            "tone_suggestions": []
        }
        
        # æ ¹æ®åˆ†ç±»æ¨èçŸ­å›å¤ï¼ˆä¼˜åŒ–ï¼‰
        category_replies = {
            "æ±‚åŠ©é—®ç­”": ["è¯•è¯•çœ‹", "æœ‰ç”¨", "åŠ æ²¹", "æ”¯æŒ", "æ²¡é—®é¢˜", "å¯ä»¥çš„", "ğŸ‘"],
            "æŠ€æœ¯è®¨è®º": ["å­¦ä¹ äº†", "æœ‰é“ç†", "èµåŒ", "æ”¶è—", "ä¸é”™", "ğŸ‘"],
            "ç”Ÿæ´»åˆ†äº«": ["æœ‰æ„æ€", "èµ", "åŒæ„Ÿ", "ä¸é”™", "ğŸ˜Š"],
            "è®¨è®ºäº¤æµ": ["åŒæ„", "æ”¯æŒ", "æœ‰é“ç†", "è®¤åŒ", "ğŸ‘"],
            "æ–°é—»èµ„è®¯": ["å…³æ³¨", "æ”¶è—", "æœ‰ç”¨", "ğŸ‘"],
            "èµ„æºåˆ†äº«": ["æ„Ÿè°¢", "æ”¶è—", "æœ‰ç”¨", "ğŸ‘"]
        }
        
        if analysis.category in category_replies:
            suggestions["short_replies"] = category_replies[analysis.category]
        
        # æ ¹æ®æƒ…æ„Ÿæ¨èè¡¨æƒ…
        if analysis.sentiment == "positive":
            suggestions["emoji_suggestions"] = ["ğŸ‘", "ğŸ˜Š", "â¤ï¸", "ğŸ”¥", "ğŸ’ª"]
        elif analysis.sentiment == "negative":
            suggestions["emoji_suggestions"] = ["ğŸ˜…", "ğŸ’ª", "ğŸ¤”"]
        else:
            suggestions["emoji_suggestions"] = ["ğŸ‘", "ğŸ¤”", "ğŸ˜Š"]
        
        # æ ¹æ®è¯­è¨€é£æ ¼æ¨èè¯­è°ƒ
        if analysis.language_style == "formal":
            suggestions["tone_suggestions"] = ["æ„Ÿè°¢åˆ†äº«", "å­¦ä¹ äº†", "å—ç›ŠåŒªæµ…"]
        elif analysis.language_style == "casual":
            suggestions["tone_suggestions"] = ["å“ˆå“ˆ", "ä¸é”™", "èµ"]
        else:
            suggestions["tone_suggestions"] = ["æ”¯æŒ", "åŒæ„", "æœ‰é“ç†"]
        
        return suggestions


if __name__ == "__main__":
    # æµ‹è¯•å†…å®¹åˆ†æå™¨
    analyzer = ContentAnalyzer()
    
    test_posts = [
        {
            "title": "Pythonçˆ¬è™«é—®é¢˜æ±‚åŠ©",
            "content": "æˆ‘åœ¨å†™çˆ¬è™«çš„æ—¶å€™é‡åˆ°äº†åçˆ¬è™«æœºåˆ¶ï¼Œæœ‰ä»€ä¹ˆå¥½çš„è§£å†³æ–¹æ¡ˆå—ï¼Ÿ"
        },
        {
            "title": "ä»Šå¤©å¤©æ°”çœŸå¥½",
            "content": "é˜³å…‰æ˜åªšï¼Œå¿ƒæƒ…ä¹Ÿå˜å¥½äº†ï¼Œå¤§å®¶ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿ"
        },
        {
            "title": "React 19æ–°ç‰¹æ€§è®¨è®º",
            "content": "React 19æ­£å¼å‘å¸ƒï¼Œå¸¦æ¥äº†å¾ˆå¤šæ–°ç‰¹æ€§ï¼Œå¤§å®¶æ€ä¹ˆçœ‹ï¼Ÿ"
        }
    ]
    
    for i, post in enumerate(test_posts, 1):
        print(f"\n=== æµ‹è¯•å¸–å­ {i} ===")
        print(f"æ ‡é¢˜: {post['title']}")
        print(f"å†…å®¹: {post['content']}")
        
        analysis = analyzer.analyze(post['title'], post['content'])
        print(f"\nåˆ†æç»“æœ:")
        print(f"  åˆ†ç±»: {analysis.category}")
        print(f"  æƒ…æ„Ÿ: {analysis.sentiment}")
        print(f"  å…³é”®è¯: {analysis.keywords}")
        print(f"  ä¸»é¢˜: {analysis.topics}")
        print(f"  å¤æ‚åº¦: {analysis.complexity}")
        print(f"  æ„å›¾: {analysis.intent}")
        print(f"  è¯­è¨€é£æ ¼: {analysis.language_style}")
        print(f"  ç½®ä¿¡åº¦: {analysis.confidence:.2f}")
        
        suggestions = analyzer.get_reply_suggestions(analysis)
        print(f"\nå›å¤å»ºè®®:")
        print(f"  çŸ­å›å¤: {suggestions['short_replies']}")
        print(f"  è¡¨æƒ…å»ºè®®: {suggestions['emoji_suggestions']}")
        print(f"  è¯­è°ƒå»ºè®®: {suggestions['tone_suggestions']}")